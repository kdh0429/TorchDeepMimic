--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 22.3     |
|    ep_reward_mean       | 0.294    |
|    mimic_body_reward    | 0.00736  |
|    mimic_body_vel_re... | 0.000409 |
|    mimic_ee_reward      | 0.000207 |
|    mimic_qpos_reward    | 0.000108 |
|    mimic_qvel_reward    | 1.06e-44 |
| time/                   |          |
|    fps                  | 196      |
|    iterations           | 1        |
|    time_elapsed         | 20       |
|    total_timesteps      | 4096     |
--------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 22.3         |
|    ep_reward_mean       | 0.303        |
|    mimic_body_reward    | 0.00563      |
|    mimic_body_vel_re... | 3.89e-05     |
|    mimic_ee_reward      | 8.7e-05      |
|    mimic_qpos_reward    | 0.000893     |
|    mimic_qvel_reward    | 2.11e-39     |
| time/                   |              |
|    fps                  | 190          |
|    iterations           | 2            |
|    time_elapsed         | 42           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 2.139248e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -43          |
|    explained_variance   | 0.0766       |
|    learning_rate        | 5e-06        |
|    loss                 | 0.0914       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00328     |
|    std                  | 1.57         |
|    value_loss           | 0.227        |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 21.7          |
|    ep_reward_mean       | 0.245         |
|    mimic_body_reward    | 0.0076        |
|    mimic_body_vel_re... | 0.000299      |
|    mimic_ee_reward      | 4.54e-05      |
|    mimic_qpos_reward    | 0.000588      |
|    mimic_qvel_reward    | 2.52e-31      |
| time/                   |               |
|    fps                  | 187           |
|    iterations           | 3             |
|    time_elapsed         | 65            |
|    total_timesteps      | 12288         |
| train/                  |               |
|    approx_kl            | -0.0001414502 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | 0.0568        |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0492        |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.00337      |
|    std                  | 1.57          |
|    value_loss           | 0.143         |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 21           |
|    ep_reward_mean       | 0.291        |
|    mimic_body_reward    | 0.0115       |
|    mimic_body_vel_re... | 0.000368     |
|    mimic_ee_reward      | 0.000169     |
|    mimic_qpos_reward    | 0.000776     |
|    mimic_qvel_reward    | 3.08e-36     |
| time/                   |              |
|    fps                  | 186          |
|    iterations           | 4            |
|    time_elapsed         | 87           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0001278203 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -43          |
|    explained_variance   | 0.0391       |
|    learning_rate        | 5e-06        |
|    loss                 | 0.0334       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00351     |
|    std                  | 1.57         |
|    value_loss           | 0.105        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 19.9         |
|    ep_reward_mean       | 0.305        |
|    mimic_body_reward    | 0.0117       |
|    mimic_body_vel_re... | 0.000253     |
|    mimic_ee_reward      | 5.29e-05     |
|    mimic_qpos_reward    | 0.00123      |
|    mimic_qvel_reward    | 2e-49        |
| time/                   |              |
|    fps                  | 185          |
|    iterations           | 5            |
|    time_elapsed         | 110          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0001828596 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -43          |
|    explained_variance   | 0.00237      |
|    learning_rate        | 5e-06        |
|    loss                 | 0.0334       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00341     |
|    std                  | 1.57         |
|    value_loss           | 0.0852       |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 20.9          |
|    ep_reward_mean       | 0.32          |
|    mimic_body_reward    | 0.0105        |
|    mimic_body_vel_re... | 0.000296      |
|    mimic_ee_reward      | 6.05e-05      |
|    mimic_qpos_reward    | 0.000292      |
|    mimic_qvel_reward    | 5.57e-49      |
| time/                   |               |
|    fps                  | 184           |
|    iterations           | 6             |
|    time_elapsed         | 132           |
|    total_timesteps      | 24576         |
| train/                  |               |
|    approx_kl            | 0.00034682453 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | -0.004        |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0329        |
|    n_updates            | 50            |
|    policy_gradient_loss | -0.0033       |
|    std                  | 1.57          |
|    value_loss           | 0.077         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 20.1           |
|    ep_reward_mean       | 0.311          |
|    mimic_body_reward    | 0.00626        |
|    mimic_body_vel_re... | 6.99e-05       |
|    mimic_ee_reward      | 5.23e-07       |
|    mimic_qpos_reward    | 0.0002         |
|    mimic_qvel_reward    | 1.13e-52       |
| time/                   |                |
|    fps                  | 184            |
|    iterations           | 7              |
|    time_elapsed         | 155            |
|    total_timesteps      | 28672          |
| train/                  |                |
|    approx_kl            | -2.7944334e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -43            |
|    explained_variance   | 0.00536        |
|    learning_rate        | 5e-06          |
|    loss                 | 0.0285         |
|    n_updates            | 60             |
|    policy_gradient_loss | -0.00356       |
|    std                  | 1.57           |
|    value_loss           | 0.0722         |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 20.6          |
|    ep_reward_mean       | 0.296         |
|    mimic_body_reward    | 0.007         |
|    mimic_body_vel_re... | 1.05e-05      |
|    mimic_ee_reward      | 2.82e-05      |
|    mimic_qpos_reward    | 0.00026       |
|    mimic_qvel_reward    | 9.82e-53      |
| time/                   |               |
|    fps                  | 184           |
|    iterations           | 8             |
|    time_elapsed         | 177           |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | -0.0001883572 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | 0.0238        |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0252        |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.00336      |
|    std                  | 1.57          |
|    value_loss           | 0.0696        |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 21.7          |
|    ep_reward_mean       | 0.327         |
|    mimic_body_reward    | 0.0171        |
|    mimic_body_vel_re... | 5.84e-05      |
|    mimic_ee_reward      | 2.08e-05      |
|    mimic_qpos_reward    | 0.00013       |
|    mimic_qvel_reward    | 5.95e-38      |
| time/                   |               |
|    fps                  | 184           |
|    iterations           | 9             |
|    time_elapsed         | 199           |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 0.00037570763 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | -0.013        |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0187        |
|    n_updates            | 80            |
|    policy_gradient_loss | -0.00342      |
|    std                  | 1.57          |
|    value_loss           | 0.0624        |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 21.7           |
|    ep_reward_mean       | 0.326          |
|    mimic_body_reward    | 0.0112         |
|    mimic_body_vel_re... | 1.01e-05       |
|    mimic_ee_reward      | 1.37e-05       |
|    mimic_qpos_reward    | 0.000348       |
|    mimic_qvel_reward    | 2.98e-50       |
| time/                   |                |
|    fps                  | 183            |
|    iterations           | 10             |
|    time_elapsed         | 222            |
|    total_timesteps      | 40960          |
| train/                  |                |
|    approx_kl            | 0.000119213015 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -43            |
|    explained_variance   | -0.0224        |
|    learning_rate        | 5e-06          |
|    loss                 | 0.0241         |
|    n_updates            | 90             |
|    policy_gradient_loss | -0.00346       |
|    std                  | 1.57           |
|    value_loss           | 0.0603         |
--------------------------------------------
Traceback (most recent call last):
  File "main.py", line 84, in <module>
    main()
  File "main.py", line 40, in main
    model.learn(total_timesteps=60000000)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 264, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 272, in learn
    wandb_dict["time_elapsed"] = t_start - t_first_start
NameError: name 't_start' is not defined
