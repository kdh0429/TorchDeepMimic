--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 22       |
|    ep_reward_mean       | 7.25     |
|    mimic_body_reward    | 0.0629   |
|    mimic_body_vel_re... | 0.00436  |
|    mimic_contact_reward | 0.046    |
|    mimic_ee_reward      | 0.0146   |
|    mimic_qpos_reward    | 0.183    |
|    mimic_qvel_reward    | 0        |
| time/                   |          |
|    fps                  | 273      |
|    iterations           | 1        |
|    time_elapsed         | 14       |
|    total_timesteps      | 4096     |
--------------------------------------
/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22.2      |
|    ep_reward_mean       | 7.45      |
|    mimic_body_reward    | 0.0768    |
|    mimic_body_vel_re... | 0.00758   |
|    mimic_contact_reward | 0.04      |
|    mimic_ee_reward      | 0.015     |
|    mimic_qpos_reward    | 0.195     |
|    mimic_qvel_reward    | 0         |
| time/                   |           |
|    fps                  | 275       |
|    iterations           | 2         |
|    time_elapsed         | 29        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | nan       |
|    clip_fraction        | 0.0158    |
|    clip_range           | 0.2       |
|    entropy_loss         | 4.38      |
|    explained_variance   | -1.82e+05 |
|    learning_rate        | 1e-05     |
|    learning_rate_critic | 0.01      |
|    loss                 | 0.373     |
|    n_updates            | 3         |
|    policy_gradient_loss | -0.0073   |
|    std                  | 0.2       |
|    value_loss           | 1.59      |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 22.1     |
|    ep_reward_mean       | 7.44     |
|    mimic_body_reward    | 0.0736   |
|    mimic_body_vel_re... | 0.0113   |
|    mimic_contact_reward | 0.034    |
|    mimic_ee_reward      | 0.0104   |
|    mimic_qpos_reward    | 0.181    |
|    mimic_qvel_reward    | 0        |
| time/                   |          |
|    fps                  | 278      |
|    iterations           | 3        |
|    time_elapsed         | 44       |
|    total_timesteps      | 12288    |
| train/                  |          |
|    approx_kl            | nan      |
|    clip_fraction        | 0.00741  |
|    clip_range           | 0.2      |
|    entropy_loss         | 4.38     |
|    explained_variance   | -0.168   |
|    learning_rate        | 1e-05    |
|    learning_rate_critic | 0.01     |
|    loss                 | 0.606    |
|    n_updates            | 6        |
|    policy_gradient_loss | -0.00562 |
|    std                  | 0.2      |
|    value_loss           | 1.34     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 22.4     |
|    ep_reward_mean       | 7.52     |
|    mimic_body_reward    | 0.0826   |
|    mimic_body_vel_re... | 0.0124   |
|    mimic_contact_reward | 0.064    |
|    mimic_ee_reward      | 0.0129   |
|    mimic_qpos_reward    | 0.176    |
|    mimic_qvel_reward    | 0        |
| time/                   |          |
|    fps                  | 280      |
|    iterations           | 4        |
|    time_elapsed         | 58       |
|    total_timesteps      | 16384    |
| train/                  |          |
|    approx_kl            | nan      |
|    clip_fraction        | 0.0106   |
|    clip_range           | 0.2      |
|    entropy_loss         | 4.38     |
|    explained_variance   | 0.519    |
|    learning_rate        | 1e-05    |
|    learning_rate_critic | 0.01     |
|    loss                 | 0.766    |
|    n_updates            | 9        |
|    policy_gradient_loss | -0.00618 |
|    std                  | 0.2      |
|    value_loss           | 1.36     |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 23.2     |
|    ep_reward_mean       | 7.68     |
|    mimic_body_reward    | 0.0858   |
|    mimic_body_vel_re... | 0.0102   |
|    mimic_contact_reward | 0.046    |
|    mimic_ee_reward      | 0.0161   |
|    mimic_qpos_reward    | 0.184    |
|    mimic_qvel_reward    | 0        |
| time/                   |          |
|    fps                  | 281      |
|    iterations           | 5        |
|    time_elapsed         | 72       |
|    total_timesteps      | 20480    |
| train/                  |          |
|    approx_kl            | nan      |
|    clip_fraction        | 0.0292   |
|    clip_range           | 0.2      |
|    entropy_loss         | 4.38     |
|    explained_variance   | 0.577    |
|    learning_rate        | 1e-05    |
|    learning_rate_critic | 0.01     |
|    loss                 | 0.57     |
|    n_updates            | 12       |
|    policy_gradient_loss | -0.00893 |
|    std                  | 0.2      |
|    value_loss           | 1.39     |
--------------------------------------
Traceback (most recent call last):
  File "main.py", line 84, in <module>
    main()
  File "main.py", line 40, in main
    model.learn(total_timesteps=60000000)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 318, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 225, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 165, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 150, in step
    return self.step_wait()
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/vec_env/vec_normalize.py", line 113, in step_wait
    obs, rews, news, infos = self.venv.step_wait()
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py", line 115, in step_wait
    results = [remote.recv() for remote in self.remotes]
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py", line 115, in <listcomp>
    results = [remote.recv() for remote in self.remotes]
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
