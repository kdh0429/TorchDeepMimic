--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 22.3     |
|    ep_reward_mean       | 11.3     |
|    mimic_body_orient... | 0.0319   |
|    mimic_body_reward    | 0.134    |
|    mimic_body_vel_re... | 0.00965  |
|    mimic_contact_reward | 0.058    |
|    mimic_qpos_reward    | 0.288    |
|    mimic_qvel_reward    | 0        |
| time/                   |          |
|    fps                  | 373      |
|    iterations           | 1        |
|    time_elapsed         | 10       |
|    total_timesteps      | 4096     |
--------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.7        |
|    ep_reward_mean       | 11          |
|    mimic_body_orient... | 0.0267      |
|    mimic_body_reward    | 0.121       |
|    mimic_body_vel_re... | 0.00953     |
|    mimic_contact_reward | 0.048       |
|    mimic_qpos_reward    | 0.274       |
|    mimic_qvel_reward    | 0           |
| time/                   |             |
|    fps                  | 365         |
|    iterations           | 2           |
|    time_elapsed         | 22          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.006265416 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | 4.38        |
|    explained_variance   | -3.91e+05   |
|    learning_rate        | 1e-05       |
|    loss                 | 8.51        |
|    n_updates            | 3           |
|    policy_gradient_loss | -0.00827    |
|    std                  | 0.2         |
|    value_loss           | 17          |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 23.2         |
|    ep_reward_mean       | 11.7         |
|    mimic_body_orient... | 0.0316       |
|    mimic_body_reward    | 0.136        |
|    mimic_body_vel_re... | 0.00775      |
|    mimic_contact_reward | 0.04         |
|    mimic_qpos_reward    | 0.283        |
|    mimic_qvel_reward    | 0            |
| time/                   |              |
|    fps                  | 369          |
|    iterations           | 3            |
|    time_elapsed         | 33           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0061956896 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | 4.38         |
|    explained_variance   | -9.4e+04     |
|    learning_rate        | 1e-05        |
|    loss                 | 7.93         |
|    n_updates            | 6            |
|    policy_gradient_loss | -0.0055      |
|    std                  | 0.2          |
|    value_loss           | 15.8         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.6        |
|    ep_reward_mean       | 11.3        |
|    mimic_body_orient... | 0.0264      |
|    mimic_body_reward    | 0.139       |
|    mimic_body_vel_re... | 0.0108      |
|    mimic_contact_reward | 0.048       |
|    mimic_qpos_reward    | 0.297       |
|    mimic_qvel_reward    | 0           |
| time/                   |             |
|    fps                  | 371         |
|    iterations           | 4           |
|    time_elapsed         | 44          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.007497946 |
|    clip_fraction        | 0.00903     |
|    clip_range           | 0.2         |
|    entropy_loss         | 4.38        |
|    explained_variance   | -3.24e+04   |
|    learning_rate        | 1e-05       |
|    loss                 | 8.06        |
|    n_updates            | 9           |
|    policy_gradient_loss | -0.00607    |
|    std                  | 0.2         |
|    value_loss           | 16.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 22.3         |
|    ep_reward_mean       | 11.4         |
|    mimic_body_orient... | 0.0181       |
|    mimic_body_reward    | 0.127        |
|    mimic_body_vel_re... | 0.00541      |
|    mimic_contact_reward | 0.024        |
|    mimic_qpos_reward    | 0.272        |
|    mimic_qvel_reward    | 0            |
| time/                   |              |
|    fps                  | 372          |
|    iterations           | 5            |
|    time_elapsed         | 54           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0043131593 |
|    clip_fraction        | 0.00163      |
|    clip_range           | 0.2          |
|    entropy_loss         | 4.38         |
|    explained_variance   | -1.37e+04    |
|    learning_rate        | 1e-05        |
|    loss                 | 7            |
|    n_updates            | 12           |
|    policy_gradient_loss | -0.00439     |
|    std                  | 0.2          |
|    value_loss           | 16.1         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 22.5         |
|    ep_reward_mean       | 11.4         |
|    mimic_body_orient... | 0.0239       |
|    mimic_body_reward    | 0.149        |
|    mimic_body_vel_re... | 0.0105       |
|    mimic_contact_reward | 0.032        |
|    mimic_qpos_reward    | 0.279        |
|    mimic_qvel_reward    | 0            |
| time/                   |              |
|    fps                  | 370          |
|    iterations           | 6            |
|    time_elapsed         | 66           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0071909763 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | 4.38         |
|    explained_variance   | -7.2e+03     |
|    learning_rate        | 1e-05        |
|    loss                 | 8.03         |
|    n_updates            | 15           |
|    policy_gradient_loss | -0.00697     |
|    std                  | 0.2          |
|    value_loss           | 16           |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 21.7         |
|    ep_reward_mean       | 11.1         |
|    mimic_body_orient... | 0.0103       |
|    mimic_body_reward    | 0.137        |
|    mimic_body_vel_re... | 0.00629      |
|    mimic_contact_reward | 0.036        |
|    mimic_qpos_reward    | 0.288        |
|    mimic_qvel_reward    | 0            |
| time/                   |              |
|    fps                  | 370          |
|    iterations           | 7            |
|    time_elapsed         | 77           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0041292375 |
|    clip_fraction        | 0.00675      |
|    clip_range           | 0.2          |
|    entropy_loss         | 4.38         |
|    explained_variance   | -4.22e+03    |
|    learning_rate        | 1e-05        |
|    loss                 | 8.54         |
|    n_updates            | 18           |
|    policy_gradient_loss | -0.00548     |
|    std                  | 0.2          |
|    value_loss           | 16.4         |
------------------------------------------
Traceback (most recent call last):
  File "main.py", line 86, in <module>
  File "main.py", line 40, in main
    # model.learn(total_timesteps=60000000)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 251, in learn
    self.train()
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 174, in train
    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/policies.py", line 623, in evaluate_actions
    log_prob = distribution.log_prob(actions)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/distributions.py", line 165, in log_prob
    log_prob = self.distribution.log_prob(actions)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/torch/distributions/normal.py", line 74, in log_prob
    var = (self.scale ** 2)
KeyboardInterrupt
