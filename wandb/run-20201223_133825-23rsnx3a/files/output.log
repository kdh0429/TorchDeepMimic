--------------------------------------
| rollout/                |          |
|    mimic_body_reward    | 0.25     |
|    mimic_body_vel_re... | 0.0999   |
|    mimic_contact_reward | 0.124    |
|    mimic_ee_reward      | 0        |
|    mimic_qpos_reward    | 0.45     |
|    mimic_qvel_reward    | 0        |
| time/                   |          |
|    fps                  | 1510     |
|    iterations           | 1        |
|    time_elapsed         | 2        |
|    total_timesteps      | 4096     |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    mimic_body_reward    | 0.25      |
|    mimic_body_vel_re... | 0.0999    |
|    mimic_contact_reward | 0.122     |
|    mimic_ee_reward      | 0         |
|    mimic_qpos_reward    | 0.45      |
|    mimic_qvel_reward    | 0         |
| time/                   |           |
|    fps                  | 1420      |
|    iterations           | 2         |
|    time_elapsed         | 5         |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0081539 |
|    clip_fraction        | 0.0299    |
|    clip_range           | 0.2       |
|    entropy_loss         | 4.38      |
|    explained_variance   | -5.77e+05 |
|    learning_rate        | 1e-05     |
|    loss                 | 120       |
|    n_updates            | 3         |
|    policy_gradient_loss | -0.00674  |
|    std                  | 0.2       |
|    value_loss           | 233       |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    mimic_body_reward    | 0.25         |
|    mimic_body_vel_re... | 0.0999       |
|    mimic_contact_reward | 0.122        |
|    mimic_ee_reward      | 0            |
|    mimic_qpos_reward    | 0.45         |
|    mimic_qvel_reward    | 0            |
| time/                   |              |
|    fps                  | 1372         |
|    iterations           | 3            |
|    time_elapsed         | 8            |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0022616712 |
|    clip_fraction        | 8.14e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | 4.38         |
|    explained_variance   | -9.93e+04    |
|    learning_rate        | 1e-05        |
|    loss                 | 115          |
|    n_updates            | 6            |
|    policy_gradient_loss | -0.00267     |
|    std                  | 0.2          |
|    value_loss           | 233          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    mimic_body_reward    | 0.25        |
|    mimic_body_vel_re... | 0.0999      |
|    mimic_contact_reward | 0.13        |
|    mimic_ee_reward      | 0           |
|    mimic_qpos_reward    | 0.45        |
|    mimic_qvel_reward    | 0           |
| time/                   |             |
|    fps                  | 1365        |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.004403742 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    entropy_loss         | 4.38        |
|    explained_variance   | -3.46e+04   |
|    learning_rate        | 1e-05       |
|    loss                 | 117         |
|    n_updates            | 9           |
|    policy_gradient_loss | -0.00642    |
|    std                  | 0.2         |
|    value_loss           | 232         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    mimic_body_reward    | 0.25        |
|    mimic_body_vel_re... | 0.0999      |
|    mimic_contact_reward | 0.124       |
|    mimic_ee_reward      | 0           |
|    mimic_qpos_reward    | 0.45        |
|    mimic_qvel_reward    | 0           |
| time/                   |             |
|    fps                  | 1360        |
|    iterations           | 5           |
|    time_elapsed         | 15          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.005876206 |
|    clip_fraction        | 0.00895     |
|    clip_range           | 0.2         |
|    entropy_loss         | 4.38        |
|    explained_variance   | -1.58e+04   |
|    learning_rate        | 1e-05       |
|    loss                 | 119         |
|    n_updates            | 12          |
|    policy_gradient_loss | -0.00507    |
|    std                  | 0.2         |
|    value_loss           | 232         |
-----------------------------------------
Traceback (most recent call last):
  File "test.py", line 84, in <module>
    main()
  File "test.py", line 40, in main
    model.learn(total_timesteps=60000000)
  File "/home/kim/anaconda3/envs/torch_rl_test/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 315, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kim/anaconda3/envs/torch_rl_test/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 222, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/kim/anaconda3/envs/torch_rl_test/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 153, in collect_rollouts
    actions, values, log_probs = self.policy.forward(obs_tensor)
  File "/home/kim/anaconda3/envs/torch_rl_test/lib/python3.6/site-packages/stable_baselines3/common/policies.py", line 549, in forward
    distribution = self._get_action_dist_from_latent(latent_pi, latent_sde=latent_sde)
  File "/home/kim/anaconda3/envs/torch_rl_test/lib/python3.6/site-packages/stable_baselines3/common/policies.py", line 584, in _get_action_dist_from_latent
    return self.action_dist.proba_distribution(mean_actions, self.log_std)
  File "/home/kim/anaconda3/envs/torch_rl_test/lib/python3.6/site-packages/stable_baselines3/common/distributions.py", line 153, in proba_distribution
    action_std = th.ones_like(mean_actions) * log_std.exp()
KeyboardInterrupt
