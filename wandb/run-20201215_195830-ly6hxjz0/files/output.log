--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20.7     |
|    ep_reward_mean       | 0.303    |
|    mimic_body_reward    | 0.0121   |
|    mimic_body_vel_re... | 7.16e-06 |
|    mimic_ee_reward      | 0.000182 |
|    mimic_qpos_reward    | 0.00105  |
|    mimic_qvel_reward    | 2.06e-38 |
| time/                   |          |
|    fps                  | 195      |
|    iterations           | 1        |
|    time_elapsed         | 20       |
|    total_timesteps      | 4096     |
--------------------------------------
Traceback (most recent call last):
  File "main.py", line 84, in <module>
    main()
  File "main.py", line 40, in main
    model.learn(total_timesteps=60000000)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 300, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 249, in learn
    self.train()
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 268, in train
    wandb_dict["time_elapsed"] = self.t_start - t_first_start
AttributeError: 'PPO' object has no attribute 't_start'
