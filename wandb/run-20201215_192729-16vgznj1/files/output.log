--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 22.3     |
|    ep_reward_mean       | 0.232    |
|    mimic_body_reward    | 0.000498 |
|    mimic_body_vel_re... | 0.000671 |
|    mimic_ee_reward      | 1.23e-07 |
|    mimic_qpos_reward    | 0.00036  |
|    mimic_qvel_reward    | 3.87e-38 |
| time/                   |          |
|    fps                  | 185      |
|    iterations           | 1        |
|    time_elapsed         | 22       |
|    total_timesteps      | 4096     |
--------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 23.5           |
|    ep_reward_mean       | 0.275          |
|    mimic_body_reward    | 0.0123         |
|    mimic_body_vel_re... | 8.99e-06       |
|    mimic_ee_reward      | 0.0006         |
|    mimic_qpos_reward    | 0.000578       |
|    mimic_qvel_reward    | 3.53e-41       |
| time/                   |                |
|    fps                  | 182            |
|    iterations           | 2              |
|    time_elapsed         | 44             |
|    total_timesteps      | 8192           |
| train/                  |                |
|    approx_kl            | -0.00029170886 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -43            |
|    explained_variance   | 0.0262         |
|    learning_rate        | 5e-06          |
|    loss                 | 0.0607         |
|    n_updates            | 10             |
|    policy_gradient_loss | -0.00379       |
|    std                  | 1.57           |
|    value_loss           | 0.203          |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 21.6          |
|    ep_reward_mean       | 0.272         |
|    mimic_body_reward    | 0.00996       |
|    mimic_body_vel_re... | 0.000193      |
|    mimic_ee_reward      | 3.69e-07      |
|    mimic_qpos_reward    | 0.000161      |
|    mimic_qvel_reward    | 4.38e-51      |
| time/                   |               |
|    fps                  | 183           |
|    iterations           | 3             |
|    time_elapsed         | 67            |
|    total_timesteps      | 12288         |
| train/                  |               |
|    approx_kl            | 0.00028506573 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | 0.00883       |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0444        |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.00309      |
|    std                  | 1.57          |
|    value_loss           | 0.125         |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 20.6          |
|    ep_reward_mean       | 0.298         |
|    mimic_body_reward    | 0.0106        |
|    mimic_body_vel_re... | 0.00209       |
|    mimic_ee_reward      | 0.000147      |
|    mimic_qpos_reward    | 0.000102      |
|    mimic_qvel_reward    | 3.42e-49      |
| time/                   |               |
|    fps                  | 183           |
|    iterations           | 4             |
|    time_elapsed         | 89            |
|    total_timesteps      | 16384         |
| train/                  |               |
|    approx_kl            | 0.00024794322 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | 0.0209        |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0353        |
|    n_updates            | 30            |
|    policy_gradient_loss | -0.00326      |
|    std                  | 1.57          |
|    value_loss           | 0.104         |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 20.4          |
|    ep_reward_mean       | 0.3           |
|    mimic_body_reward    | 0.0126        |
|    mimic_body_vel_re... | 8.07e-06      |
|    mimic_ee_reward      | 0.000358      |
|    mimic_qpos_reward    | 0.00107       |
|    mimic_qvel_reward    | 1.91e-45      |
| time/                   |               |
|    fps                  | 183           |
|    iterations           | 5             |
|    time_elapsed         | 111           |
|    total_timesteps      | 20480         |
| train/                  |               |
|    approx_kl            | 1.4573336e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | 0.0169        |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0341        |
|    n_updates            | 40            |
|    policy_gradient_loss | -0.00326      |
|    std                  | 1.57          |
|    value_loss           | 0.0865        |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 22.3          |
|    ep_reward_mean       | 0.334         |
|    mimic_body_reward    | 0.0142        |
|    mimic_body_vel_re... | 0.000312      |
|    mimic_ee_reward      | 0.000508      |
|    mimic_qpos_reward    | 0.000518      |
|    mimic_qvel_reward    | 1.55e-32      |
| time/                   |               |
|    fps                  | 184           |
|    iterations           | 6             |
|    time_elapsed         | 133           |
|    total_timesteps      | 24576         |
| train/                  |               |
|    approx_kl            | 0.00019988231 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | -0.00992      |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0295        |
|    n_updates            | 50            |
|    policy_gradient_loss | -0.00329      |
|    std                  | 1.57          |
|    value_loss           | 0.0769        |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 21.4           |
|    ep_reward_mean       | 0.285          |
|    mimic_body_reward    | 0.0278         |
|    mimic_body_vel_re... | 8.1e-05        |
|    mimic_ee_reward      | 9.9e-05        |
|    mimic_qpos_reward    | 0.000665       |
|    mimic_qvel_reward    | 6.49e-47       |
| time/                   |                |
|    fps                  | 184            |
|    iterations           | 7              |
|    time_elapsed         | 155            |
|    total_timesteps      | 28672          |
| train/                  |                |
|    approx_kl            | -0.00011622533 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -43            |
|    explained_variance   | -0.0314        |
|    learning_rate        | 5e-06          |
|    loss                 | 0.0295         |
|    n_updates            | 60             |
|    policy_gradient_loss | -0.00365       |
|    std                  | 1.57           |
|    value_loss           | 0.0743         |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 19.6          |
|    ep_reward_mean       | 0.295         |
|    mimic_body_reward    | 0.0191        |
|    mimic_body_vel_re... | 0.00084       |
|    mimic_ee_reward      | 0.00027       |
|    mimic_qpos_reward    | 0.000693      |
|    mimic_qvel_reward    | 1.71e-56      |
| time/                   |               |
|    fps                  | 184           |
|    iterations           | 8             |
|    time_elapsed         | 177           |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | -0.0002711583 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | 0.0355        |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0263        |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.00342      |
|    std                  | 1.57          |
|    value_loss           | 0.0665        |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 21.5          |
|    ep_reward_mean       | 0.316         |
|    mimic_body_reward    | 0.0156        |
|    mimic_body_vel_re... | 0.000272      |
|    mimic_ee_reward      | 7.29e-07      |
|    mimic_qpos_reward    | 9.85e-05      |
|    mimic_qvel_reward    | 7.13e-33      |
| time/                   |               |
|    fps                  | 183           |
|    iterations           | 9             |
|    time_elapsed         | 200           |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 0.00042625517 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | -0.0158       |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0228        |
|    n_updates            | 80            |
|    policy_gradient_loss | -0.00355      |
|    std                  | 1.57          |
|    value_loss           | 0.0638        |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 21.9          |
|    ep_reward_mean       | 0.339         |
|    mimic_body_reward    | 0.0215        |
|    mimic_body_vel_re... | 0.000363      |
|    mimic_ee_reward      | 8.83e-05      |
|    mimic_qpos_reward    | 0.000289      |
|    mimic_qvel_reward    | 5.74e-43      |
| time/                   |               |
|    fps                  | 183           |
|    iterations           | 10            |
|    time_elapsed         | 222           |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00023974758 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -43           |
|    explained_variance   | 0.00642       |
|    learning_rate        | 5e-06         |
|    loss                 | 0.0205        |
|    n_updates            | 90            |
|    policy_gradient_loss | -0.00365      |
|    std                  | 1.57          |
|    value_loss           | 0.0609        |
-------------------------------------------
Traceback (most recent call last):
  File "main.py", line 84, in <module>
    main()
  File "main.py", line 40, in main
    model.learn(total_timesteps=60000000)
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py", line 264, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/kim/anaconda3/envs/torch_rl/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 276, in learn
    for (loss_val, loss_name) in zip(loss_vals, self.loss_names):
NameError: name 'loss_vals' is not defined
